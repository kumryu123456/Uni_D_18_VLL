import os
import json
import re
from glob import glob
from typing import Dict, Any, List, Tuple

import torch
from torch.utils.data import Dataset, DataLoader
from PIL import Image
import pandas as pd
from tqdm import tqdm

from transformers import (
    AutoConfig,
    AutoProcessor,
    AutoModelForCausalLM,
)

# ==============================
# ê¸°ë³¸ ì„¤ì •
# ==============================
DEVICE = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
MODEL_ID = "microsoft/Florence-2-large-ft"  # í•™ìŠµ ë•Œ ì¼ë˜ base ëª¨ë¸ ID


# ==============================
# ìœ í‹¸ í•¨ìˆ˜ë“¤
# ==============================
def seed_everything(seed: int = 42):
    import random
    import numpy as np

    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)


def read_json(path: str) -> Dict[str, Any]:
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)


def get_image_path(json_path: str, data: Dict[str, Any], jpg_root: str) -> str:
    """
    MI3 json â†’ MI2 jpg ê²½ë¡œ ì°¾ê¸°.
    trainì—ì„œ ì‚¬ìš©í•˜ë˜ ê²ƒê³¼ ë™ì¼í•œ ë¡œì§ì„ testì— ë§ê²Œ ì¬ì‚¬ìš©.
    - jpg_root: ./data/test/images  ê°™ì€ ë””ë ‰í† ë¦¬
    """
    src = data.get("source_data_info", {})
    jpg_name = src.get("source_data_name_jpg", None)

    # case 1: ë©”íƒ€ë°ì´í„°ì— jpg_nameì´ ëª…ì‹œë˜ì–´ ìˆëŠ” ê²½ìš°
    if jpg_name:
        cand = os.path.join(jpg_root, jpg_name)
        if os.path.exists(cand):
            return cand

        # json ê²½ë¡œ ê¸°ì¤€ìœ¼ë¡œ json â†’ jpg í´ë” ì¹˜í™˜ (í˜¹ì‹œ êµ¬ì¡°ê°€ ë¹„ìŠ·í•œ ê²½ìš° ëŒ€ë¹„)
        maybe = json_path.replace(os.sep + "json" + os.sep, os.sep + "jpg" + os.sep)
        maybe = os.path.join(os.path.dirname(maybe), jpg_name)
        if os.path.exists(maybe):
            return maybe

        # MI3 â†’ MI2 ì´ë¦„ë§Œ ë°”ê¿”ì„œ ì‹œë„
        base = os.path.basename(json_path)  # ì˜ˆ: MI3_000001.json
        jpg_base = base.replace("MI3", "MI2").rsplit(".", 1)[0] + ".jpg"
        sibling = os.path.join(jpg_root, jpg_base)
        if os.path.exists(sibling):
            return sibling

    # case 2: ë©”íƒ€ë°ì´í„°ê°€ ì—†ìœ¼ë©´ íŒŒì¼ëª… ê¸°ë°˜ìœ¼ë¡œ ë§¤ì¹­
    base = os.path.basename(json_path)
    stem = os.path.splitext(base)[0]  # ì˜ˆ: 000001.json â†’ 000001
    cand1 = os.path.join(jpg_root, stem + ".jpg")
    cand2 = os.path.join(jpg_root, stem.replace("MI3", "MI2") + ".jpg")

    if os.path.exists(cand1):
        return cand1
    if os.path.exists(cand2):
        return cand2

    raise FileNotFoundError(f"[get_image_path] JPG not found for json={json_path}")


def parse_florence_output_to_bbox(
    text: str, img_w: int, img_h: int
) -> Tuple[float, float, float, float]:
    """
    Florence-2 ì¶œë ¥ì—ì„œ <loc_?> í† í° 4ê°œë¥¼ íŒŒì‹±í•´ì„œ
    ì‹¤ì œ ì´ë¯¸ì§€ ì¢Œí‘œê³„(x, y, w, h)ë¡œ ë³€í™˜.
    """
    matches = re.findall(r"<loc_(\d+)>", text)
    if len(matches) < 4:
        # ì‹¤íŒ¨ ì‹œ: ì´ë¯¸ì§€ ì¤‘ì•™ì— ì ë‹¹í•œ ë°•ìŠ¤
        return img_w / 4, img_h / 4, img_w / 2, img_h / 2

    lx1, ly1, lx2, ly2 = map(int, matches[:4])
    x1 = lx1 / 999 * img_w
    y1 = ly1 / 999 * img_h
    x2 = lx2 / 999 * img_w
    y2 = ly2 / 999 * img_h
    return x1, y1, x2 - x1, y2 - y1


def is_visual_ann(a: Dict[str, Any]) -> bool:
    """
    trainì—ì„œ ì“°ë˜ ê²ƒê³¼ ë¹„ìŠ·í•˜ê²Œ,
    ì°¨íŠ¸/í‘œ ë“± + visual_instruction ìˆëŠ” ê²ƒë§Œ ì‚¬ìš©.
    """
    cid = str(a.get("class_id", "") or "")
    cname = str(a.get("class_name", "") or "")
    has_q = bool(str(a.get("visual_instruction", "") or "").strip())
    looks_visual = cid.startswith("V") or any(
        k in cname for k in ["í‘œ", "ì°¨íŠ¸", "ê·¸ë˜í”„", "chart", "table"]
    )
    return has_q and looks_visual


# ==============================
# ëª¨ë¸ ë¡œë”
# ==============================
def load_finetuned_model(model_dir: str):
    """
    - config/êµ¬ì¡°ëŠ” í•­ìƒ base MODEL_IDì—ì„œ ê°€ì ¸ì˜¤ê³ 
    - weightë§Œ model_dir(checkpoint)ì—ì„œ ë¡œë“œí•œë‹¤.
    ì´ë ‡ê²Œ í•´ì•¼ vision_config assertion ì—ëŸ¬ë¥¼ í”¼í•  ìˆ˜ ìˆìŒ.
    """
    print(f"[load_finetuned_model] base model: {MODEL_ID}")
    print(f"[load_finetuned_model] finetuned weights from: {model_dir}")

    # 1) base config + ëª¨ë¸ êµ¬ì¡°
    base_config = AutoConfig.from_pretrained(
        MODEL_ID,
        trust_remote_code=True,
    )

    model = AutoModelForCausalLM.from_pretrained(
        MODEL_ID,
        config=base_config,
        trust_remote_code=True,
        torch_dtype=torch.float32,  # ì¶”ë¡ ì€ fp32ë¡œ ì•ˆì „í•˜ê²Œ
    ).to(DEVICE)

    # 2) fine-tuned ê°€ì¤‘ì¹˜ ë¡œë“œ
    weight_path_bin = os.path.join(model_dir, "pytorch_model.bin")
    weight_path_sf = os.path.join(model_dir, "model.safetensors")

    if os.path.exists(weight_path_bin):
        state_dict = torch.load(weight_path_bin, map_location="cpu")
        print(f"  - loaded weights from {weight_path_bin}")
    elif os.path.exists(weight_path_sf):
        from safetensors.torch import load_file

        state_dict = load_file(weight_path_sf)
        print(f"  - loaded weights from {weight_path_sf}")
    else:
        raise FileNotFoundError(
            f"No weights found in {model_dir} (expected pytorch_model.bin or model.safetensors)"
        )

    missing, unexpected = model.load_state_dict(state_dict, strict=False)
    print(f"  - missing keys   : {len(missing)}")
    print(f"  - unexpected keys: {len(unexpected)}")

    # 3) processorëŠ” fine-tune ì‹œ ì €ì¥í•œ ë””ë ‰í† ë¦¬ì—ì„œ ë¡œë“œ
    processor = AutoProcessor.from_pretrained(
        model_dir,
        trust_remote_code=True,
    )

    return model, processor


# ==============================
# Test Dataset
# ==============================
class FlorenceTestDataset(Dataset):
    """
    test ë””ë ‰í† ë¦¬ êµ¬ì¡° ê°€ì •:
      data/test/
        â”œâ”€ query/   : *.json (MI3_....json)
        â””â”€ images/  : *.jpg  (MI2_....jpg)

    - json_path + get_image_path()ë¡œ ì´ë¯¸ì§€ ê²½ë¡œë¥¼ ì°¾ëŠ”ë‹¤.
    - query_id  : annotation.instance_id
    - query_text: annotation.visual_instruction
    """

    def __init__(self, test_dir: str):
        json_dir = os.path.join(test_dir, "query")
        jpg_root = os.path.join(test_dir, "images")

        json_files = sorted(glob(os.path.join(json_dir, "*.json")))
        self.samples: List[Dict[str, Any]] = []

        if not json_files:
            print(f"[FlorenceTestDataset] No json files found in {json_dir}")

        for jf in json_files:
            try:
                data = read_json(jf)
            except Exception as e:
                print(f"[WARN] failed to read {jf}: {e}")
                continue

            # ì´ë¯¸ì§€ ê²½ë¡œ (json í•˜ë‚˜ë‹¹ ì´ë¯¸ì§€ í•˜ë‚˜ë¼ê³  ê°€ì •)
            try:
                img_path = get_image_path(jf, data, jpg_root)
            except FileNotFoundError as e:
                # print(f"[WARN] {e}")
                continue

            # annotation ë¦¬ìŠ¤íŠ¸ì—ì„œ visualí•œ ê²ƒë§Œ ì‚¬ìš©
            ann_list = (
                data.get("learning_data_info", {})
                .get("annotation", [])
            )

            for ann in ann_list:
                if not is_visual_ann(ann):
                    continue

                instance_id = str(ann.get("instance_id", "") or "").strip()
                qtext = str(ann.get("visual_instruction", "") or "").strip()

                if not instance_id or not qtext:
                    continue

                self.samples.append(
                    {
                        "query_id": instance_id,          # ğŸ”¹ CSVìš©
                        "query_text": qtext,              # ğŸ”¹ CSVìš©
                        "question_for_model": "<CAPTION_TO_PHRASE_GROUNDING>" + qtext,
                        "image_path": img_path,
                    }
                )

        print(f"[TestDataset] Loaded {len(self.samples)} items")

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        s = self.samples[idx]
        img = Image.open(s["image_path"]).convert("RGB")
        img_w, img_h = img.size

        meta = {
            "img_size": (img_w, img_h),
            "image_path": s["image_path"],
            "query_text": s["query_text"],
        }

        # ëª¨ë¸ ì…ë ¥: (query_id, question_for_model, image, meta)
        return s["query_id"], s["question_for_model"], img, meta


def make_collate_fn(processor):
    def collate_fn(batch):
        qids, questions, images, metas = zip(*batch)
        inputs = processor(
            text=list(questions),
            images=list(images),
            return_tensors="pt",
            padding=True,
        )
        return list(qids), inputs, list(metas)

    return collate_fn


# ==============================
# Inference ë£¨í”„
# ==============================
def run_test(model, processor, loader: DataLoader, output_csv: str = "./submission.csv"):
    model.eval()
    results = []

    with torch.no_grad():
        pbar = tqdm(loader, desc="Test inference")

        for qids, inputs, metas in pbar:
            # deviceë¡œ ì˜¬ë¦¬ê¸°
            for k, v in inputs.items():
                if isinstance(v, torch.Tensor):
                    inputs[k] = v.to(DEVICE)

            # pixel_values dtype ë§ì¶”ê¸°
            if "pixel_values" in inputs:
                inputs["pixel_values"] = inputs["pixel_values"].to(model.dtype)

            # Florence-2 generate
            gen_ids = model.generate(
                input_ids=inputs["input_ids"],
                pixel_values=inputs["pixel_values"],
                max_new_tokens=32,
                num_beams=3,
                do_sample=False,
                pad_token_id=processor.tokenizer.pad_token_id,
                eos_token_id=processor.tokenizer.eos_token_id,
            )

            texts = processor.batch_decode(gen_ids, skip_special_tokens=False)

            for qid, text, meta in zip(qids, texts, metas):
                img_w, img_h = meta["img_size"]
                x, y, w, h = parse_florence_output_to_bbox(text, img_w, img_h)

                results.append(
                    {
                        "query_id": qid,
                        "query_text": meta["query_text"],  # ğŸ”¹ CSVì— visual_instruction ê·¸ëŒ€ë¡œ
                        "pred_x": x,
                        "pred_y": y,
                        "pred_w": w,
                        "pred_h": h,
                    }
                )

    # ì»¬ëŸ¼ ìˆœì„œ ëª…ì‹œ
    df = pd.DataFrame(
        results,
        columns=["query_id", "query_text", "pred_x", "pred_y", "pred_w", "pred_h"],
    )
    df.to_csv(output_csv, index=False)
    print(f"[Done] Saved submission to {output_csv}")


# ==============================
# main
# ==============================
def main():
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument("--test_dir", type=str, default="./data/test")
    parser.add_argument("--model_dir", type=str, default="./outputs/florence2_bbox/best")
    parser.add_argument("--batch_size", type=int, default=16)
    parser.add_argument("--output_csv", type=str, default="./my_submission.csv")
    args = parser.parse_args()

    seed_everything(42)

    # 1) ëª¨ë¸ / í”„ë¡œì„¸ì„œ ë¡œë“œ
    model, processor = load_finetuned_model(args.model_dir)

    # 2) Dataset / DataLoader
    test_ds = FlorenceTestDataset(args.test_dir)
    loader = DataLoader(
        test_ds,
        batch_size=args.batch_size,
        shuffle=False,
        num_workers=0,
        collate_fn=make_collate_fn(processor),
    )

    # 3) Inference & CSV ì €ì¥
    run_test(model, processor, loader, args.output_csv)


if __name__ == "__main__":
    main()